{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python NLP Tutorial \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial was designed as a training tool for the HERC NLP Team and serves as a basic introduction to:\n",
    "\n",
    "* Python programming concepts\n",
    "* Implementation of natural language processing techniques (NLP)\n",
    "* Popular Python libraries and data constructs used in data analytics\n",
    "* NLP applications towards healthcare\n",
    "\n",
    "This tutorial assumes basic understanding of programming techniques, syntax, and structures, whether in Python or another language.\n",
    "\n",
    "### Tools\n",
    "\n",
    "This tutorial uses an application called [Jupyter Notebook](https://jupyter.org) as the programming interface. Jupyter is an interactive development environment that allows users to create and modify dynamic code \"cells\" and run them individually. This allows for easy experimentation and testing of code. The cells below have instructions and guidelines on how to manipulate the code for various results. You are also free to experiment on your own.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Each of these sections of text or code represent a cell. You will see the cell outlined when you click on it.\n",
    "* Any text that is [underlined]() is a link, usually to a reference page that provides more information about a given module or function.\n",
    "* A cell can be run by pressing `Ctrl + Enter` or by clicking the \"Run\" button on the toolbar above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "The first step in a Python program is to declare any external modules that will be used by the code. A module is a name for any file that contains Python code. A group of modules is often referred to as a library. A lot of the work we need has already been accomplished by others, so we can just download and reference their code (modules) for our own programs. We will be using the following modules:\n",
    "\n",
    "### re\n",
    "The [`re`](https://docs.python.org/3/library/re.html) module will allow us to search text using regular expressions, an approach to pattern recognition. These will be explained in greater depth below.\n",
    "\n",
    "### pandas\n",
    "The [`pandas`](https://pandas.pydata.org) module provides objects and methods that will allow us to efficiently structure and analyze large data sets. The primary structure is called a DataFrame, a 2D tabular object that allows for high performance of complex operations. A user-friendly tutorial can be found here: https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare an import using the keyword 'import'\n",
    "# Click on this cell to select it and press 'ctrl + enter' to run\n",
    "\n",
    "import re\n",
    "import pandas as pd  # we use the 'as' keyword to declare a shortcut to reference the pandas module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Input\n",
    "\n",
    "The first step to analyzing data is to input the data. We will be working with an extract from the [FDA MAUDE database](https://www.fda.gov/medical-devices/mandatory-reporting-requirements-manufacturers-importers-and-device-user-facilities/manufacturer-and-user-facility-device-experience-database-maude), which features adverse event reports involving medical devices. We are interested in reports related to EPIC software, so we have pulled a dataset of reports that contain the word \"EPIC\".\n",
    "\n",
    "We will read our data from a CSV file ('EPIC_MAUDE.csv') and load it into our program using the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. This will translate the file into a DataFrame that can be accessed for our analysis.\n",
    "\n",
    "After we read the data, we can view the output to make sure it's formatted correctly. In Jupyter Notebook, the final command of a cell will display its output (if applicable) every time you run that cell. We thus use the final command in the below cell to display the data using bracket notation. Python, like most mainstream languages, is 0-indexed, meaning that the first element of a list is referenced with the number \\[0\\], the second with \\[1\\], and so on. With a pandas DataFrame, we can access a subsection, or \"slice\" of the data, using colon notation `[start_index:stop_index]`. Play around with the example below using different values and observe what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error running this cell, make sure you've run the imports cell first\n",
    "\n",
    "data = pd.read_csv('EPIC_MAUDE.csv')\n",
    "#print(data[0:10])  # uncomment this line by removing the first '#' symbol at the beginning\n",
    "data[0:10]  # try entering different numbers and re-running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the [`print`](https://docs.python.org/3/library/functions.html#print) function to output data from any part of a cell. Try to uncomment the line `print(data[0:10])` in the cell above and re-run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the Data\n",
    "\n",
    "### Regular Expressions Intro\n",
    "The next step is to closer examine the data. We do some preliminary investigations to see what is notable or interesting about the data, anything that stands out or presents a feasible direction for our analysis. For quantitative data, this would often involve descriptive statistics, histograms, and the like, but for text based data, the approach is more driven by the content of the data or the type of problem we are trying to solve. In this case, we are looking at reports related to EPIC, so let's look for occurrences of the word \"Epic\".\n",
    "\n",
    "We will use a search pattern technique called [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). The [`search`](https://docs.python.org/3/library/re.html#re.search) function allows us to search a text string for a given pattern. It has two required arguments: 1) the search pattern and 2) the text to be searched. It then returns a [match object](https://docs.python.org/3/library/re.html#match-objects) that contains information about any matches in the text string. Searching is case sensitive by default, so we can circumvent this issue by making our pattern and search text both lowercase. In the cell below, we set our pattern to search for the string \"epic\". We will select the first record in our data file as the search string. Run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'epic'\n",
    "text = data['ed_text'][0].lower()  # notation to read the first record of the 'ed_text' column and lowercase the text\n",
    "match = re.search(pattern, text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Match Objects\n",
    "\n",
    "When the code above runs, it prints the match object that is returned. The segment `span=(316, 320)` tells us that the search pattern was located between character 316 and 320 in text.\n",
    "\n",
    "For this to be practically useful, we can create a function that prints the surrounding text, just to give us the context that the word was used in. It will take two inputs, 1) a match object and 2) the window size (denoting how much surrounding text to show). The function will then output a string that contains the matched word and its context based on the inputs. We can then use a loop to examine several records this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(match, window=25):  # window size variable defaults to 25 if not explicitly given\n",
    "    start_index = max(0, match.start() - window)  # to prevent a negative number assignment\n",
    "    end_index = match.end() + window\n",
    "    \n",
    "    return match.string[start_index:end_index]\n",
    "\n",
    "print(context(match, 20))  # try different values for the window size variable to see what a helpful window looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'epic'\n",
    "matches = []  # create an empty list\n",
    "\n",
    "# iterate through reports and return a list of match objects (for positive matches) \n",
    "for text in data['ed_text']:  # loop through entire 'ed_text' column\n",
    "    match = re.search(pattern, text.lower())\n",
    "    if match is not None:\n",
    "        matches.append(match)  # add the match object to the list\n",
    "            \n",
    "for match in matches[:25]:  # loops through first 25 matches, change this value if you like\n",
    "    print(context(match))  # add a second argument for window size or change the default (and rerun the cell) in the function above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper, Part I\n",
    "\n",
    "We can immediately see that there are several reports we are not interested in. Since we searched for the term \"epic\", we are seeing reports related to \"Epic vascular stent system\" and other types of medical devices. Here is where we begin to see the power of regular expressions. We can use them to make our search more precise, so it picks up occurrences of the word \"epic\" that are not followed by terms related to the vascular stent system. Looking at the output from above, we can see a few words we may want to filter out-- \"vascular\", \"stent\", \"valve\", etc.\n",
    "\n",
    "We can get this done using a _negative lookahead assertion_, which matches if the following text does not match the given expression. This means we can look for text that matches \"epic\", but does _not_ match \"vascular\", \"stent\", or \"valve\". The syntax for this assertion is `(?!<expression>)`. We can identify multiple words to avoid using the `|` character. **Replace line 1 in the above code cell** with\n",
    "\n",
    "    pattern = 'epic (?!stent|valve|vascular|sds|implant)'\n",
    "    \n",
    "and rerun the code to see what happens. Try to add or remove terms from the expression to see how it changes the results.\n",
    "\n",
    "Additional resources:\n",
    "* a [reference page](https://docs.python.org/3/library/re.html#regular-expression-syntax) for regular expression syntax.\n",
    "* a [helpful tutorial](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial) on regular expressions in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper, Part II\n",
    "\n",
    "You can see that the negative lookahead assertion removed some of the unwanted reports, but still leaves a lot of noise. We can keep adding terms to the assertion but if we look closely at the results, we see that there are often additional words between \"epic\" and one of our filter terms (e.g. \"epic self-expanding stents\", \"epic heart valve\"). So a cleaner approach would be to account for this word gap, regardless of what the interceding words are. A quick approach is to add a secondary filter step, that would remove reports that contain one of our \"negative\" keywords nearby the word \"epic\".\n",
    "\n",
    "In this case, we are creating a secondary pattern that, if matched, will pass over that report instead of adding it to our list of matches. The code for this algorithm is below. Note line 2:\n",
    "\n",
    "    negative_pattern = 'epic ([^.!?]+ ){0,5}?(stent|valve|vascular|sds|implant)'\n",
    "\n",
    "This is beginning to look pretty complicated, so let's break it down.\n",
    "* We first have `epic `, which, as before, looks for the word \"epic\", followed by a space.\n",
    "* Next is this bizarre pattern `([^.!?]* )`. This essentially matches a word, excluding any end punctuation.\n",
    "* The following `{0,5}?` tells it to match the previous pattern, `([^.!?]* )` anywhere between 0 and 5 times. This allows between zero and five words between the word \"epic\" and one of our exclusionary words, as long as there is no end punctuation.\n",
    "* Finally, we have our list of exclusionary words `(stent|valve|vascular|sds|implant)`.\n",
    "\n",
    "You can find a helpful demonstration [here](https://www.regexpal.com/?fam=111616), which highlights matched terms from a selection of reports. The link also allows you to edit the expression and see how the matched results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_pattern = 'epic'\n",
    "negative_pattern = 'epic ([^.!? ]+ ){0,5}?(stent|valve|vascular|sds|implant)'\n",
    "matches = []\n",
    "\n",
    "# iterate through reports and return a list of match objects (for positive matches) \n",
    "for text in data['ed_text']:\n",
    "    match = re.search(positive_pattern, text.lower())\n",
    "    if match is not None:\n",
    "        if not re.search(negative_pattern, text.lower()):\n",
    "            matches.append(match)  # add the match object to the list\n",
    "                        \n",
    "for match in matches[:25]:\n",
    "    print(context(match, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Output the Data\n",
    "As you can see in the output above, there are still some irrelevant results, but the noise has been greatly reduced, and we can see more results related to the Epic system. There is definitely more filtering that can be done, so feel free to experiment some more. The final step is to 1) create a set of reports that meet our filtering requirements and 2) output these reports as a file. The `pandas` library provides easy functionality for these tasks.\n",
    "\n",
    "1. First, we create a function that takes a text string and returns True or False depending on whether or not it passes our test filter.\n",
    "2. Second, we use the [`apply`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) function to create a new dataset comprised of all rows where the `ed_text` field passes our function.\n",
    "3. Finally, we write our filtered dataset to a CSV file using the [`to_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) function.\n",
    "\n",
    "Run the following cell. If you look at the output file, you will see that we've reduced our set from 1500 reports to 196! From here, we can tweak our code further, create additional filters, or start producing a training set for a machine learning algorithm. This code can be applied to any additional dataset that is similar to the original input.\n",
    "\n",
    "**Note**: If you a running this notebook on the server (using Binder), you can access the output file by going back to the directory (where you opened this file) and clicking on the file. Once the file is open, you can go to \"File\" > \"Download\" to save a copy to your computer. You can also download this notebook as a Python file by going to \"File\" > \"Download as\" > \"Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1 ###\n",
    "def is_match(text):\n",
    "    text = text.lower()\n",
    "    positive_pattern = 'epic'\n",
    "    negative_pattern = 'epic ([^.!? ]+ ){0,5}?(stent|valve|vascular|sds|implant)'\n",
    "    \n",
    "    if re.search(positive_pattern, text):  # check for match of term \"epic\"\n",
    "        if not re.search(negative_pattern, text.lower()):  # check against exclusionary pattern\n",
    "            return True  # indicates that the text passes the filter\n",
    "        \n",
    "    return False  # default to False if text does not pass filter\n",
    "\n",
    "### Step 2 ###\n",
    "filtered_data = data[data.ed_text.apply(lambda x: is_match(x))]\n",
    "# add the following code in the line below to check the output: print(filtered_data)\n",
    "\n",
    "### Step 3 ###\n",
    "filtered_data.to_csv('EPIC_MAUDE_filtered.csv', index=False)  # feel free to change the output name/location"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
